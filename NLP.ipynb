{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting text from website and Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "response = urllib.request.urlopen(\"http://en.wikipedia.org/wiki/Natural_language_processing\")\n",
    "html = response.read()\n",
    "soup = BeautifulSoup(html,\"html5lib\") \n",
    "text = soup.get_text(strip=True) \n",
    "tokens = [t for t in text.split()] \n",
    "#print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize Text Using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time is endless in thy hands, my lord.', 'There is none to count thy minutes.', 'Days and nights pass and ages bloom and fade like flowers.', 'Thou knowest how to wait.', 'Thy centuries follow each other perfecting a small wild flower.', 'We have no time to lose, \\nand having no time we must scramble for a chance.', 'We are too poor to be late.', 'And thus it is that time goes by \\nwhile I give it to every querulous man who claims it, \\nand thine altar is empty of all offerings to the last.', 'At the end of the day I hasten in fear lest thy gate be shut; \\nbut I find that yet there is time.']\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize \n",
    "\n",
    "tokens = \"\"\"Time is endless in thy hands, my lord. \n",
    "There is none to count thy minutes. \n",
    "\n",
    "Days and nights pass and ages bloom and fade like flowers. \n",
    "Thou knowest how to wait. \n",
    "\n",
    "Thy centuries follow each other perfecting a small wild flower. \n",
    "\n",
    "We have no time to lose, \n",
    "and having no time we must scramble for a chance. \n",
    "We are too poor to be late. \n",
    "\n",
    "And thus it is that time goes by \n",
    "while I give it to every querulous man who claims it, \n",
    "and thine altar is empty of all offerings to the last. \n",
    "\n",
    "At the end of the day I hasten in fear lest thy gate be shut; \n",
    "but I find that yet there is time.\"\"\" \n",
    "\n",
    "print(sent_tokenize(tokens, \"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'is', 'endless', 'in', 'thy', 'hands', ',', 'my', 'lord', '.', 'There', 'is', 'none', 'to', 'count', 'thy', 'minutes', '.', 'Days', 'and', 'nights', 'pass', 'and', 'ages', 'bloom', 'and', 'fade', 'like', 'flowers', '.', 'Thou', 'knowest', 'how', 'to', 'wait', '.', 'Thy', 'centuries', 'follow', 'each', 'other', 'perfecting', 'a', 'small', 'wild', 'flower', '.', 'We', 'have', 'no', 'time', 'to', 'lose', ',', 'and', 'having', 'no', 'time', 'we', 'must', 'scramble', 'for', 'a', 'chance', '.', 'We', 'are', 'too', 'poor', 'to', 'be', 'late', '.', 'And', 'thus', 'it', 'is', 'that', 'time', 'goes', 'by', 'while', 'I', 'give', 'it', 'to', 'every', 'querulous', 'man', 'who', 'claims', 'it', ',', 'and', 'thine', 'altar', 'is', 'empty', 'of', 'all', 'offerings', 'to', 'the', 'last', '.', 'At', 'the', 'end', 'of', 'the', 'day', 'I', 'hasten', 'in', 'fear', 'lest', 'thy', 'gate', 'be', 'shut', ';', 'but', 'I', 'find', 'that', 'yet', 'there', 'is', 'time', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(word_tokenize(tokens,\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = [word for word in word_tokenize(tokens) if word.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Frequency Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor key,val in freq.items(): \\n    print (str(key) + ': ' + str(val))\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "freq = nltk.FreqDist(words) \n",
    "'''\n",
    "for key,val in freq.items(): \n",
    "    print (str(key) + ': ' + str(val))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Time', 1), ('is', 5), ('endless', 1), ('in', 2), ('thy', 3), ('hands', 1), ('my', 1), ('lord', 1), ('There', 1), ('none', 1), ('to', 6), ('count', 1), ('minutes', 1), ('Days', 1), ('and', 5), ('nights', 1), ('pass', 1), ('ages', 1), ('bloom', 1), ('fade', 1), ('like', 1), ('flowers', 1), ('Thou', 1), ('knowest', 1), ('how', 1), ('wait', 1), ('Thy', 1), ('centuries', 1), ('follow', 1), ('each', 1), ('other', 1), ('perfecting', 1), ('a', 2), ('small', 1), ('wild', 1), ('flower', 1), ('We', 2), ('have', 1), ('no', 2), ('time', 4), ('lose', 1), ('having', 1), ('we', 1), ('must', 1), ('scramble', 1), ('for', 1), ('chance', 1), ('are', 1), ('too', 1), ('poor', 1), ('be', 2), ('late', 1), ('And', 1), ('thus', 1), ('it', 3), ('that', 2), ('goes', 1), ('by', 1), ('while', 1), ('I', 3), ('give', 1), ('every', 1), ('querulous', 1), ('man', 1), ('who', 1), ('claims', 1), ('thine', 1), ('altar', 1), ('empty', 1), ('of', 2), ('all', 1), ('offerings', 1), ('the', 3), ('last', 1), ('At', 1), ('end', 1), ('day', 1), ('hasten', 1), ('fear', 1), ('lest', 1), ('gate', 1), ('shut', 1), ('but', 1), ('find', 1), ('yet', 1), ('there', 1)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:1\n",
      "endless:1\n",
      "thy:1\n",
      "hands:1\n",
      "lord:1\n",
      "There:1\n",
      "none:1\n",
      "count:1\n",
      "minutes:1\n",
      "Days:1\n",
      "nights:1\n",
      "pass:1\n",
      "ages:1\n",
      "bloom:1\n",
      "fade:1\n",
      "like:1\n",
      "flowers:1\n",
      "Thou:1\n",
      "knowest:1\n",
      "wait:1\n",
      "Thy:1\n",
      "centuries:1\n",
      "follow:1\n",
      "other:1\n",
      "perfecting:1\n",
      "small:1\n",
      "wild:1\n",
      "flower:1\n",
      "We:1\n",
      "no:1\n",
      "time:1\n",
      "lose:1\n",
      "we:1\n",
      "must:1\n",
      "scramble:1\n",
      "chance:1\n",
      "too:1\n",
      "poor:1\n",
      "late:1\n",
      "And:1\n",
      "thus:1\n",
      "that:1\n",
      "goes:1\n",
      "while:1\n",
      "I:1\n",
      "give:1\n",
      "every:1\n",
      "querulous:1\n",
      "man:1\n",
      "claims:1\n",
      "thine:1\n",
      "altar:1\n",
      "empty:1\n",
      "all:1\n",
      "offerings:1\n",
      "last:1\n",
      "At:1\n",
      "end:1\n",
      "day:1\n",
      "hasten:1\n",
      "fear:1\n",
      "lest:1\n",
      "gate:1\n",
      "shut:1\n",
      "find:1\n",
      "yet:1\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokens = list(freq.keys())\n",
    "\n",
    "for token in tokens:\n",
    "    if token in stopwords.words('english'):\n",
    "        tokens.remove(token)\n",
    "\n",
    "freq = nltk.FreqDist(tokens) \n",
    "\n",
    "for key,val in freq.items(): \n",
    "    print (str(key) + ':' + str(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2c31a526f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "freq.plot(20,cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enjoying or showing or marked by joy or pleasure\n",
      "['a happy smile', 'spent many happy days on the beach', 'a happy marriage']\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "syn = wordnet.synsets(\"happy\")\n",
    "print(syn[0].definition())\n",
    "print(syn[0].examples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['natural_language_processing', 'NLP', 'human_language_technology']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet \n",
    "\n",
    "synonyms = []\n",
    "for syn in wordnet.synsets('NLP'):\n",
    "    for lemma in syn.lemmas():\n",
    "        synonyms.append(lemma.name())\n",
    "        \n",
    "print(synonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['evil', 'evilness', 'bad', 'badness', 'bad', 'evil', 'ill']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "antonyms = []\n",
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "print(antonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Stemming\n",
    "\n",
    "Removing affixes from words and returning the root word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "print(stemmer.stem('working'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "# Languages supported by SnowballStemmer\n",
    "\n",
    "print(SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watch\n"
     ]
    }
   ],
   "source": [
    "french_stemmer = SnowballStemmer('english')\n",
    "\n",
    "print(french_stemmer.stem(\"watching\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words Lemmatizing \n",
    "\n",
    "Similar to stemming, but give us a real word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing: increase\n",
      "       vs\n",
      "Stemming: increas\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(\"Lemmatizing: \" + lemmatizer.lemmatize('increases'))\n",
    "print(\"       vs\")\n",
    "print(\"Stemming: \" + stemmer.stem('increases'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "playing\n",
      "playing\n",
      "playing\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize('playing', pos=\"v\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"n\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"a\")) \n",
    "print(lemmatizer.lemmatize('playing', pos=\"r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
